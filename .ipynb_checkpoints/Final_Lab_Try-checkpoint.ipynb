{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 CSV 数据\n",
    "file_path = \"df.csv\"  # 修改为实际路径\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取舞蹈流派、视频编号和摄像头编号\n",
    "data['dance_type'] = data['images'].apply(lambda x: x.split('/')[1].split('_')[0])  # 舞蹈流派\n",
    "data['video_id'] = data['images'].apply(lambda x: x.split('/')[1].split('_')[1])   # 视频编号\n",
    "data['camera_id'] = data['images'].apply(lambda x: x.split('/')[1].split('_')[2])  # 摄像头编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dance_type  num_videos\n",
      "0     HipHop           6\n",
      "1       Jazz           6\n",
      "2       Kata           6\n",
      "3     Taichi           6\n"
     ]
    }
   ],
   "source": [
    "# 按舞蹈流派和视频编号统计\n",
    "dance_video_stats = data.groupby(['dance_type', 'video_id']).size().reset_index(name='frame_count')\n",
    "\n",
    "# 按流派统计视频数量\n",
    "dance_type_summary = dance_video_stats.groupby('dance_type')['video_id'].count().reset_index(name='num_videos')\n",
    "\n",
    "# 输出统计结果\n",
    "print(dance_type_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集划分： {'HipHop': 4, 'Jazz': 4, 'Kata': 4, 'Taichi': 4}\n",
      "测试集划分： {'HipHop': 2, 'Jazz': 2, 'Kata': 2, 'Taichi': 2}\n"
     ]
    }
   ],
   "source": [
    "# 定义函数进行视频划分（按70%训练集和30%测试集划分）\n",
    "def split_videos(dance_video_stats, train_ratio=0.7):\n",
    "    train_videos, test_videos = {}, {}\n",
    "    for dance_type in dance_video_stats['dance_type'].unique():\n",
    "        videos = dance_video_stats[dance_video_stats['dance_type'] == dance_type]['video_id'].unique()\n",
    "        random.shuffle(videos)  # 随机打乱\n",
    "        split_idx = int(len(videos) * train_ratio)\n",
    "        train_videos[dance_type] = videos[:split_idx]  # 前70%为训练集\n",
    "        test_videos[dance_type] = videos[split_idx:]   # 剩余30%为测试集\n",
    "    return train_videos, test_videos\n",
    "\n",
    "# 执行划分\n",
    "train_videos, test_videos = split_videos(dance_video_stats)\n",
    "\n",
    "# 打印划分结果\n",
    "print(\"训练集划分：\", {k: len(v) for k, v in train_videos.items()})\n",
    "print(\"测试集划分：\", {k: len(v) for k, v in test_videos.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DanceDataset(Dataset):\n",
    "    def __init__(self, data, root_dir, transform=None, num_frames=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (DataFrame): 包含 images 列和 dance_type 列的 DataFrame。\n",
    "            root_dir (str): images 文件夹的根目录。\n",
    "            transform (callable, optional): 对图像进行的变换操作。\n",
    "            num_frames (int): 每个样本包含的帧数（时间维度）。\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # print(\"Dataset Preview:\")\n",
    "        # print(self.data.head())  # 检查数据内容\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.data.iloc[idx]['images'])\n",
    "        # 检查路径是否存在\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"文件未找到：{image_path}\")\n",
    "\n",
    "        # 加载图像\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 将舞蹈类别转换为整数标签\n",
    "        label = self.data.iloc[idx]['dance_type']\n",
    "        label_mapping = {\"HipHop\": 0, \"Jazz\": 1, \"Kata\": 2, \"Taichi\": 3}\n",
    "        label = label_mapping[label]\n",
    "\n",
    "        # 添加时间维度\n",
    "        # 添加时间维度，重复图像到 num_frames\n",
    "        image = image.unsqueeze(0).repeat(self.num_frames, 1, 1, 1)  # (time, height, width)\n",
    "        # image = image.unsqueeze(0)  # (1, height, width)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 711\n",
      "测试集样本数: 383\n"
     ]
    }
   ],
   "source": [
    "# 展平 train_videos 和 test_videos 的所有视频 ID 列表\n",
    "train_video_ids = [video for videos in train_videos.values() for video in videos]\n",
    "test_video_ids = [video for videos in test_videos.values() for video in videos]\n",
    "\n",
    "# 过滤训练集数据\n",
    "train_data = data[data['video_id'].isin(train_video_ids)]\n",
    "test_data = data[data['video_id'].isin(test_video_ids)]\n",
    "\n",
    "# 打印训练集和测试集样本数\n",
    "print(f\"训练集样本数: {len(train_data)}\")\n",
    "print(f\"测试集样本数: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图像变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((250, 250)),  # 调整图像大小\n",
    "    transforms.ToTensor(),         # 转换为 Tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 归一化\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "root_dir = \"segmentation_full_body_mads_dataset\"\n",
    "train_dataset = DanceDataset(train_data, root_dir=root_dir, transform=transform)\n",
    "test_dataset = DanceDataset(test_data, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([16, 5, 3, 250, 250])\n",
      "Batch labels: tensor([1, 1, 2, 0, 3, 0, 2, 3, 3, 0, 3, 0, 0, 3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation_full_body_mads_dataset/images/HipHop_HipHop1_C0_00180.png\n",
      "segmentation_full_body_mads_dataset/images/HipHop_HipHop1_C0_00225.png\n",
      "segmentation_full_body_mads_dataset/images/HipHop_HipHop1_C0_00360.png\n",
      "segmentation_full_body_mads_dataset/images/HipHop_HipHop1_C0_00405.png\n",
      "segmentation_full_body_mads_dataset/images/HipHop_HipHop1_C0_00450.png\n"
     ]
    }
   ],
   "source": [
    "# 检查 train_data 的图像路径拼接是否正确\n",
    "for i in range(5):  # 查看前5条路径\n",
    "    print(os.path.join(root_dir, train_data.iloc[i]['images']))\n",
    "\n",
    "# 实例化数据集\n",
    "train_dataset = DanceDataset(train_data, root_dir=root_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像尺寸: <built-in method size of Tensor object at 0x15bd971d0>\n",
      "标签: 0\n"
     ]
    }
   ],
   "source": [
    "# 测试数据加载\n",
    "dataset = DanceDataset(train_data, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# 检查第一个样本\n",
    "image, label = dataset[0]\n",
    "print(f\"图像尺寸: {image.size}\")  # 打印图像尺寸\n",
    "print(f\"标签: {label}\")         # 打印标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像批次维度: torch.Size([16, 5, 3, 250, 250])\n",
      "标签批次: tensor([1, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 3, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 检查数据加载器是否正常工作\n",
    "for images, labels in train_loader:\n",
    "    print(f\"图像批次维度: {images.shape}\")\n",
    "    print(f\"标签批次: {labels}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dance3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Dance3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 1, 1)) \n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 1 * 124 * 124, 128) \n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Input to conv1: {x.shape}\") \n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print(f\"After conv1: {x.shape}\")\n",
    "        x = self.pool1(x)\n",
    "        # print(f\"After pool1: {x.shape}\")\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print(f\"After conv2: {x.shape}\")\n",
    "        x = self.pool2(x)\n",
    "        # print(f\"After pool2: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(f\"After view: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Dance3DCNN(num_classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "        # for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.permute(0, 2, 1, 3, 4)  # (batch_size, channels, time, height, width)\n",
    "            # (batch_size, channels*time, height, width)\n",
    "            # images = images.reshape(images.size(0), 3, images.size(2), images.size(3), images.size(4))\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # train_loss /= len(train_loader)\n",
    "        train_accuracy = 100. * correct / total\n",
    "        train_losses.append(train_loss / len(train_loader))  # 平均损失\n",
    "        # train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            # for images, labels in tqdm(test_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                images = images.permute(0, 2, 1, 3, 4)  # 验证也需要调整维度\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "      \n",
    "        # test_loss /= len(test_loader)\n",
    "        test_accuracy = 100. * correct / total\n",
    "        val_losses.append(test_loss / len(test_loader))  # 平均损失\n",
    "        val_accuracies.append(test_accuracy)\n",
    "        print(f\"Validation Loss: {test_loss:.4f}, Validation Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "        # 打印学习率\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n",
    "            \n",
    "\n",
    "    # 绘制 Loss 和 Accuracy 曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([16, 5, 3, 250, 250])\n",
      "Batch labels: tensor([1, 1, 1, 2, 2, 3, 1, 0, 3, 1, 2, 0, 2, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 1/5, Train Loss: 33.5016, Train Accuracy: 69.90%\n",
      "Validation Loss: 6.0508, Validation Accuracy: 89.56%\n",
      "Learning Rate: 0.0001\n",
      "Epoch 2/5\n",
      "Epoch 2/5, Train Loss: 7.3478, Train Accuracy: 95.22%\n",
      "Validation Loss: 1.4891, Validation Accuracy: 99.22%\n",
      "Learning Rate: 0.0001\n",
      "Epoch 3/5\n",
      "Epoch 3/5, Train Loss: 2.7033, Train Accuracy: 99.02%\n",
      "Validation Loss: 0.9476, Validation Accuracy: 98.96%\n",
      "Learning Rate: 0.0001\n",
      "Epoch 4/5\n",
      "Epoch 4/5, Train Loss: 1.3725, Train Accuracy: 99.58%\n",
      "Validation Loss: 3.8173, Validation Accuracy: 92.43%\n",
      "Learning Rate: 0.0001\n",
      "Epoch 5/5\n",
      "Epoch 5/5, Train Loss: 1.2376, Train Accuracy: 99.58%\n",
      "Validation Loss: 0.5194, Validation Accuracy: 99.48%\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxiaoewochaoshini.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 绘制 Loss 和 Accuracy 曲线\u001b[39;00m\n\u001b[1;32m     74\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 75\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "torch.save(model.state_dict(),\"xiaoewochaoshini.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 绘制 Loss 和 Accuracy 曲线\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制 Loss 和 Accuracy 曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
